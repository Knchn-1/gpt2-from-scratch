# GPT-2 From Scratch ğŸš€

A project where I implemented GPT-2 (124M) from scratch, fine-tuned it for Spam classification, and performed instruction fine-tuning to create a lightweight personal assistant.

ğŸ¯ Overview

Built GPT-2 architecture manually: embeddings, multi-head attention, feedforward layers, residual connections, and normalization.

Loaded OpenAIâ€™s publicly released GPT-2 124M weights into the custom model.

Fine-tuned for classification and instruction-following.

Demonstrated a personal assistant using a Colab notebook.

ğŸ’¡ Key Learnings

Deep understanding of Transformers â€“ embeddings, attention, feedforward, residuals, and normalization.

Loading and adapting pretrained weights â€“ integrating GPT-2 124M weights into a custom model.

Fine-tuning techniques â€“ classification tasks and instruction-following.

Working with limited hardware â€“ memory and computation optimization.

End-to-end model building â€“ from raw tokens to meaningful outputs, understanding every intermediate step.

ğŸ™ Credits

OpenAI â€“ for publicly releasing GPT-2 weights.

Vizuara Team & Dr. Raj Dandekar â€“ for the â€œBuilding LLM from Scratchâ€ playlist and guidance:

Dr. Raj Dandekar

Dr. Rajat Dandekar

Dr. Sreedath Panat

Sahil Pocker

Abhijeet Singh

Sourav Jana

âš¡ Next Steps

Experiment with larger GPT-2 models (355M, 774M) on cloud GPUs.

Add evaluation metrics and visualizations.

Continuously update the Colab notebook with improvements.
