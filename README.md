# GPT-2 From Scratch 🚀

A project where I implemented GPT-2 (124M) from scratch, fine-tuned it for Spam classification, and performed instruction fine-tuning to create a lightweight personal assistant.

🎯 Overview

Built GPT-2 architecture manually: embeddings, multi-head attention, feedforward layers, residual connections, and normalization.

Loaded OpenAI’s publicly released GPT-2 124M weights into the custom model.

Fine-tuned for classification and instruction-following.

Demonstrated a personal assistant using a Colab notebook.

💡 Key Learnings

Deep understanding of Transformers – embeddings, attention, feedforward, residuals, and normalization.

Loading and adapting pretrained weights – integrating GPT-2 124M weights into a custom model.

Fine-tuning techniques – classification tasks and instruction-following.

Working with limited hardware – memory and computation optimization.

End-to-end model building – from raw tokens to meaningful outputs, understanding every intermediate step.

🙏 Credits

OpenAI – for publicly releasing GPT-2 weights.

Vizuara Team & Dr. Raj Dandekar – for the “Building LLM from Scratch” playlist and guidance:

Dr. Raj Dandekar

Dr. Rajat Dandekar

Dr. Sreedath Panat

Sahil Pocker

Abhijeet Singh

Sourav Jana

⚡ Next Steps

Experiment with larger GPT-2 models (355M, 774M) on cloud GPUs.

Add evaluation metrics and visualizations.

Continuously update the Colab notebook with improvements.
