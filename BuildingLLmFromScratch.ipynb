{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCVlTKtc-2DY"
      },
      "source": [
        "# Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4ijGayf9dg-"
      },
      "source": [
        "## Test text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {
        "id": "bie2rTWdAwl8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "file_path='/content/the_verdict.txt'\n",
        "url='https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/the-verdict.txt'\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "  with urllib.request.urlopen(url) as response:\n",
        "    text_data=response.read().decode('utf-8')\n",
        "  with open(file_path,\"w\",encoding=\"utf-8\") as file:\n",
        "    file.write(text_data)\n",
        "\n",
        "else:\n",
        "  with open(file_path,\"r\",encoding=\"utf-8\") as file:\n",
        "        text_data=file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sujzWaYPAwii",
        "outputId": "39f19973-39c5-4f4e-d5db-83295d76e42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "<!DOCTYPE html>\n",
            "<html\n",
            "  lang=\"en\"\n",
            "  \n",
            "  data-color-mode=\"auto\" data-light-theme=\"light\" data-d\n",
            "ssertive\" class=\"sr-only mt-n1\" aria-live=\"assertive\" aria-atomic=\"true\"></div>\n",
            "  </body>\n",
            "</html>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text_data[:99])\n",
        "print(text_data[-99:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPQRe1ABBQEM"
      },
      "source": [
        "## Tokenizing the data using byte pair Tokenizer from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 361,
      "metadata": {
        "id": "7TZzMgG_Awf5"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "tokenizer=tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "metadata": {
        "id": "GqOi_IR_AwdT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "class GPTDatasetV1:\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.input_ids=[]\n",
        "    self.target_ids=[]\n",
        "\n",
        "    token_ids=tokenizer.encode(txt,allowed_special={'<|endoftext|>'})\n",
        "\n",
        "    for i in range(0,len(token_ids)-max_length,stride):\n",
        "      input_chunk=token_ids[i:max_length]\n",
        "      target_chunk=token_ids[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.input_ids[idx],self.target_ids[idx]\n",
        "\n",
        "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
        "\n",
        "  tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "  dataset=GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
        "\n",
        "  dataloader=DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      num_workers=num_workers,\n",
        "      drop_last=drop_last\n",
        "  )\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ctbp7G0W3Ik"
      },
      "source": [
        "d_in: input embedding dimension\n",
        "\n",
        "d_out: output dimension of queries/keys/values\n",
        "\n",
        "\n",
        "for multiHeadAttention,d_out= no. of head * head dim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeBD5hdWZEru"
      },
      "source": [
        "This is single-head, causal self-attention:\n",
        "\n",
        "Self-attention: The same sequence provides Queries, Keys, and Values (Q, K, V).\n",
        "\n",
        "Causal: Each position can only attend to itself and the past, never the future (important for autoregressive language models).\n",
        "\n",
        "Single-head: There’s just one attention head (no splitting/concatenating). Output shape is (B, T, d_out)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvvEtRZx9r9L"
      },
      "source": [
        "## Causal Attention (for understanding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 363,
      "metadata": {
        "id": "tCBJ9AtEAwaM"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,qkv_bias=False):\n",
        "     super().__init__()\n",
        "     self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias  )\n",
        "     self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "     self.W_values=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "     self.dropout = nn.Dropout(dropout)\n",
        "     self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_tokens,d_in=x.shape\n",
        "    keys=self.W_key(x)\n",
        "    queries=self.W_query(x)\n",
        "    values=self.W_values(x)\n",
        "\n",
        "    # key shape(b,num_tokens,d_out) for matrix mul wee nedd to transpose it. (1,2) meaning dim of 1 place replace dim of 2nd(b,d_out,num_token)\n",
        "    # OR attn_scores=queries.matmul(keys.T)\n",
        "    attn_scores= queries @ keys.transpose(1,2)\n",
        "    attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
        "                               )\n",
        "    attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "    attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZO6GInf9xrb"
      },
      "source": [
        "## Multihead Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "metadata": {
        "id": "KMjQ3M5vAwXg"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        # head_dim is the indiviual head dim,here d_out is the combine dim(query head 1 + query head 2)\n",
        "        #d_out=512, num_heads=8 → head_dim=64.\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH0_-qaRwxuB"
      },
      "source": [
        "### GPT-2 Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 365,
      "metadata": {
        "id": "59ZTpvl0AwUs"
      },
      "outputs": [],
      "source": [
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5upc6I0CxChL"
      },
      "source": [
        "Layer Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 366,
      "metadata": {
        "id": "VO3ezmdKAwR1"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-4AVgmKzab8"
      },
      "source": [
        "GELU Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 367,
      "metadata": {
        "id": "ZtHfMM8ew_FU"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf3Uy_vD0Mmr"
      },
      "source": [
        "Feed Forward NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 368,
      "metadata": {
        "id": "ScgcySmrw-wx"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8cIWCNC9-KS"
      },
      "source": [
        "## Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {
        "id": "N4P-9z_t1RcP"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZN9NU4D-BW3"
      },
      "source": [
        "## GPT MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {
        "id": "hVXFn_auw-tQ"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pN_1pfU5oKi",
        "outputId": "fa7040d0-96d7-46a7-a132-79a376233c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: 208861\n",
            "Tokens: 91160\n"
          ]
        }
      ],
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKzOchwC5s7E",
        "outputId": "e7c0dd1e-02b6-48fe-f596-19ba56232acd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 372
        }
      ],
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "\n",
        "    ###Input batch:\n",
        " ###tensor([[6109, 3626, 6100,  345],\n",
        "        ##[6109, 1110, 6622,  257]])\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "uoeHyO1dfKEB"
      },
      "execution_count": 373,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "ZI9yKw7he_ff"
      },
      "execution_count": 374,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "id": "kTdveCoF5s3m"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {
        "id": "wDH1N-kEw-n7"
      },
      "outputs": [],
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {
        "id": "ibzI0AQA5sen"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peI2Yery7I9e"
      },
      "source": [
        "## Loading parameters from the Open AI GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {
        "id": "JLN2bb0i7ESN"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow>=2.15.0 tqdm>=4.66"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUaAyFTH7EOp",
        "outputId": "fe9b821e-8353-4ad0-9620-8f4a7d8c1e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorfllow Version:  2.19.0\n",
            "tqdm Version:  4.67.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tqdm\n",
        "print(\"Tensorfllow Version: \",tf.__version__)\n",
        "print(\"tqdm Version: \",tqdm.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "id": "QP4sQYbp7EMH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    # Validate model size\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    # Define paths\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    # Download files\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "        download_file(file_url, file_path)\n",
        "\n",
        "\n",
        "    # Load settings and params\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
        "\n",
        "    return settings, params\n",
        "\n",
        "def download_file(url, destination):\n",
        "    try:\n",
        "        # Send a GET request to download the file, disabling SSL verification\n",
        "        response = requests.get(url, stream=True, verify=False)\n",
        "\n",
        "        # Get the total file size from headers, defaulting to 0 if not present\n",
        "        file_size = int(response.headers.get(\"content-length\", 0))\n",
        "\n",
        "        # Check if file exists and has the same size\n",
        "        if os.path.exists(destination):\n",
        "            file_size_local = os.path.getsize(destination)\n",
        "            if file_size == file_size_local:\n",
        "                print(f\"File already exists and is up-to-date: {destination}\")\n",
        "                return\n",
        "\n",
        "        # Define the block size for reading the file\n",
        "        block_size = 1024  # 1 Kilobyte\n",
        "\n",
        "        # Initialize the progress bar with total file size\n",
        "        progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
        "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
        "            # Open the destination file in binary write mode\n",
        "            with open(destination, \"wb\") as file:\n",
        "                # Iterate over the file data in chunks\n",
        "                for chunk in response.iter_content(block_size):\n",
        "                    progress_bar.update(len(chunk))  # Update progress bar\n",
        "                    file.write(chunk)  # Write the chunk to the file\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading the file: {e}\")\n",
        "        print(f\"Please check the URL: {url}\")\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
        "    # Initialize parameters dictionary with empty blocks for each layer\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Iterate over each variable in the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        # Load the variable and remove singleton dimensions\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "\n",
        "        # Process the variable name to extract relevant parts\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Identify the target dictionary for the variable\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Recursively access or create nested dictionaries\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Assign the variable array to the last key\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    return params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEP0kD5c7Wk-"
      },
      "source": [
        "**Files description:**\n",
        "\n",
        "1.   checkpoint: In this there is mode_checkpoint_path: \"model.ckpt\".It is the path or file name we are having 3 such files with this file name ,the most imp one is model.ckpt.data.This is the place where all the weights are stored.So this checkpoint only contain path.\n",
        "\n",
        "1.   encoder.json: It have vocabulary(size=50257)\n",
        "2.   vocab.bpe:Byte pair tokenizer(G there indicates the start point and endpoint of token)\n",
        "\n",
        "\n",
        "2.   hparams.json:Contains configuration of GPT-2\n",
        "\n",
        "1.   gpt_download3.py:It is file where all the files and funcition are called,here the param dictionary returned by some functiion.\n",
        "2.   param dictionary: It contains 5 keys\n",
        "\n",
        "*   wte: Token Embeddings\n",
        "*   wpe : Positional embeddings\n",
        "\n",
        "*   Blocks\n",
        "*   g:Final norm scale\n",
        "\n",
        "*   b:Final norm shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "id": "8B14e9f17EJn"
      },
      "outputs": [],
      "source": [
        "#from gpt_download3 import download_and_load_gpt2\n",
        "#Use when the above func is not as a function,instead as a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v7-kb-z7EG3",
        "outputId": "9089dccc-e558-4da1-acc6-4b0ed8fcb332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "setting , params =download_and_load_gpt2(model_size=\"124M\",models_dir=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKDoSAvO7EEH",
        "outputId": "253fd28a-adad-4290-8ba0-c7aace81c9c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings:  {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter dictionary keys:  dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ],
      "source": [
        "print(\"Settings: \",setting)\n",
        "print(\"Parameter dictionary keys: \",params.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0dfUeb47EBx",
        "outputId": "e6f4b718-7ff9-45ee-b0a3-e1668afe8d84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
            "   0.04531523]\n",
            " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
            "   0.04318958]\n",
            " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
            "  -0.08785918]\n",
            " ...\n",
            " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
            "  -0.06952604]\n",
            " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
            "  -0.02245961]\n",
            " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
            "   0.12067825]]\n",
            "Token embedding weight tensor dimensions:  (50257, 768)\n"
          ]
        }
      ],
      "source": [
        "print(params[\"wte\"])\n",
        "print(\"Token embedding weight tensor dimensions: \",params[\"wte\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "id": "yFKtpvlv73IE"
      },
      "outputs": [],
      "source": [
        "# Define model configurations in a dictionary for compactness\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "# Copy the base configuration and update with specific model settings\n",
        "model_name = \"gpt2-small (124M)\"  # Example model name\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwbJyMcA7-kZ"
      },
      "source": [
        "bias vectors are not used now because it doesnt improve models performance,but as ut was used in gpt-2,we will use here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {
        "id": "DCRE28SL73Eo"
      },
      "outputs": [],
      "source": [
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "id": "pjTRUNip73Bn"
      },
      "outputs": [],
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {
        "id": "_ivREUi572-x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"] , 3, axis=-1)  #location where the weights are\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])  #recyling the weights from token emd to\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ygXhamN728C",
        "outputId": "eb182818-c90e-4c9a-91a9-3eca22ac74d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Create a device object (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {
        "id": "7b_eQxp4725f"
      },
      "outputs": [],
      "source": [
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4KUCugW-nX0"
      },
      "source": [
        "## Output generated by GPT-2 small Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy-6JAC_8ZFR",
        "outputId": "68879276-f407-4068-bd43-6148e72a617a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Once upon a time, in a small village nestled between the hills in the province of Palau (and within close proximity of its current centre). In this small region (it has\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\n",
        "        \"Once upon a time, in a small village nestled between\"\n",
        "        , tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdXr0eRjDne"
      },
      "source": [
        "# FINETUNING FOR CLASSIFICATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVhGEWAUjHvH"
      },
      "source": [
        "### DOWNLOADING DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro0O8LF4mWLa"
      },
      "source": [
        "(project_folder)/\n",
        "\n",
        "\n",
        "│\n",
        "\n",
        "\n",
        "\n",
        "├── main.py\n",
        "\n",
        "\n",
        "├── sms_spam_collection.zip\n",
        "\n",
        "\n",
        "├── sms_spam_collection/\n",
        "\n",
        "\n",
        "│   └── SMSSpamCollection.tsv  <-- renamed file (final dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFkG1JpviYgt",
        "outputId": "05c0b145-5ba1-482a-d3ea-cc91eba29203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import ssl\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    # Create an unverified SSL context\n",
        "    ssl_context = ssl._create_unverified_context()\n",
        "\n",
        "    # Downloading the file\n",
        "    with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    # Add .tsv file extension\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path)\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "8WLkCaavmfOs",
        "outputId": "663a6d21-7384-400f-8477-a29524b9499e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Label                                               Text\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...    ...                                                ...\n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568   ham               Will ü b going to esplanade fr home?\n",
              "5569   ham  Pity, * was in mood for that. So...any other s...\n",
              "5570   ham  The guy did some bitching but I acted like i'd...\n",
              "5571   ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc046acb-b405-4114-84f5-b906909549d3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc046acb-b405-4114-84f5-b906909549d3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dc046acb-b405-4114-84f5-b906909549d3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dc046acb-b405-4114-84f5-b906909549d3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b8832873-156b-480a-a04c-cb9b9ec5abe4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b8832873-156b-480a-a04c-cb9b9ec5abe4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b8832873-156b-480a-a04c-cb9b9ec5abe4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_6e8f94f0-f00f-43eb-a2fb-b3d1ab09a3a5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6e8f94f0-f00f-43eb-a2fb-b3d1ab09a3a5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 393
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYNT7kQRmrvg",
        "outputId": "ef2fc015-d06f-4908-b2f3-a654f6008ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df[\"Label\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUYAkX-_muHd",
        "outputId": "bf5b59cf-3e0c-4356-8158-89bac24f9fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     747\n",
            "spam    747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def create_balanced_dataset(df):\n",
        "\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
        "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 396,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuC3y7QwnNLu",
        "outputId": "86b33087-3e51-409a-b0a1-19a1326c2de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "747\n"
          ]
        }
      ],
      "source": [
        "num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "print(num_spam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 397,
      "metadata": {
        "id": "LfdtZjwrnSda"
      },
      "outputs": [],
      "source": [
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 398,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "X3tzIB2Nn6mF",
        "outputId": "b6259932-9200-498b-ec0b-282160e5f2a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Label                                               Text\n",
              "4307      0  Awww dat is sweet! We can think of something t...\n",
              "4138      0                             Just got to  &lt;#&gt;\n",
              "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
              "4461      0  This is wishing you a great day. Moji told me ...\n",
              "5440      0      Thank you. do you generally date the brothas?"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1adc611-9099-46bb-a166-4ea11193a5c0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4307</th>\n",
              "      <td>0</td>\n",
              "      <td>Awww dat is sweet! We can think of something t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4138</th>\n",
              "      <td>0</td>\n",
              "      <td>Just got to  &amp;lt;#&amp;gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4831</th>\n",
              "      <td>0</td>\n",
              "      <td>The word \"Checkmate\" in chess comes from the P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4461</th>\n",
              "      <td>0</td>\n",
              "      <td>This is wishing you a great day. Moji told me ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5440</th>\n",
              "      <td>0</td>\n",
              "      <td>Thank you. do you generally date the brothas?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1adc611-9099-46bb-a166-4ea11193a5c0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1adc611-9099-46bb-a166-4ea11193a5c0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1adc611-9099-46bb-a166-4ea11193a5c0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6de32791-a676-4268-9d5f-d116d96fee07\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6de32791-a676-4268-9d5f-d116d96fee07')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6de32791-a676-4268-9d5f-d116d96fee07 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "balanced_df",
              "summary": "{\n  \"name\": \"balanced_df\",\n  \"rows\": 1494,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1388,\n        \"samples\": [\n          \"chile, please! It's only a  &lt;DECIMAL&gt;  hour drive for me. I come down all the time and will be subletting feb-april for audition season.\",\n          \"I only haf msn. It's yijue@hotmail.com\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 398
        }
      ],
      "source": [
        "balanced_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 399,
      "metadata": {
        "id": "QRHvrg4Tn-bq"
      },
      "outputs": [],
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "    # Shuffle the entire DataFrame\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "    # Calculate split indices\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    # Split the DataFrame\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "# Test size is implied to be 0.2 as the remainder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 400,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSid_qt_oFZU",
        "outputId": "16e61a64-46b0-4e93-d745-1fed8e63816c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1045\n",
            "149\n",
            "300\n"
          ]
        }
      ],
      "source": [
        "print(len(train_df))\n",
        "print(len(validation_df))\n",
        "print(len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 401,
      "metadata": {
        "id": "u5ZLyWrWpEWA"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 402,
      "metadata": {
        "id": "cIGU0ikabo5c"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "tokenizer=tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 403,
      "metadata": {
        "id": "FCUQV4UvpMNa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
        "        ]\n",
        "\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "\n",
        "            # Truncate sequences if they are longer than max_length\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[:self.max_length]\n",
        "                for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "\n",
        "        # Pad sequences to the longest sequence\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return (\n",
        "            torch.tensor(encoded, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            encoded_length = len(encoded_text)\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "        return max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 404,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehpeaFsGqJfV",
        "outputId": "12c94089-041a-4e24-bc78-295a63cef9ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ],
      "source": [
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(train_dataset.max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 405,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L2OPZBSbM-D",
        "outputId": "b1a12c5e-5cc4-45e6-b401-b3c157930d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ],
      "source": [
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(test_dataset.max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 406,
      "metadata": {
        "id": "2K4on7Xdb9fz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 407,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75P6Jplig3nb",
        "outputId": "3dc33858-f9d0-45cb-8a9a-a5073d36708a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "Input batch dimensions: torch.Size([8, 120])\n",
            "Label batch dimensions torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "for input_batch, target_batch in train_loader:\n",
        "    pass\n",
        "\n",
        "print(\"Input batch dimensions:\", input_batch.shape)\n",
        "print(\"Label batch dimensions\", target_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 408,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2AHP7EThjCr",
        "outputId": "e8db2f65-b0ea-47bb-ceba-02a4ee5c03f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130 training batches\n",
            "19 validation batches\n",
            "38 test batches\n"
          ]
        }
      ],
      "source": [
        "print(f\"{len(train_loader)} training batches\")\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "print(f\"{len(test_loader)} test batches\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 409,
      "metadata": {
        "id": "T4hims4dLu0x"
      },
      "outputs": [],
      "source": [
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\"\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
        "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
        "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
        "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 410,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwltSuWvL5Lw",
        "outputId": "107f5b8b-3a1d-4b88-9fe3-754c186bbbc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 411,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHkwBS2fL-Yw",
        "outputId": "1f65dc65-635f-47e8-c0b5-b4ea6f7b8acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every effort moves you forward.\n",
            "\n",
            "The first step is to understand the importance of your work\n"
          ]
        }
      ],
      "source": [
        "text_1 = \"Every effort moves you\"\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_1, tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 412,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scnTnDrCL-xA",
        "outputId": "5a48a696-76fa-4e77-ed4e-bc74e383ae45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
            "\n",
            "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
          ]
        }
      ],
      "source": [
        "text_2 = (\n",
        "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
        "    \" 'You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
        ")\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_2, tokenizer),\n",
        "    max_new_tokens=23,\n",
        "    context_size=BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqh-_cG_E7ve"
      },
      "source": [
        "## Adding a classification block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQJhtSjwFJBS"
      },
      "source": [
        "Finetuning will be apply only on 3 layers\n",
        "\n",
        "1.   Final Output head\n",
        "2.   Final Transformer Layer\n",
        "1.   Final Layer Normalization\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 413,
      "metadata": {
        "id": "PDzO0WHsqAnz"
      },
      "outputs": [],
      "source": [
        "#freeze all the layers\n",
        "#make the parameteres non trainable\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 414,
      "metadata": {
        "id": "mOXU6s8qMIL8"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "num_classes = 2\n",
        "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxyn2X7ZLsRp"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Additionally, we configure the last transformer block and the final LayerNorm module,\n",
        "which connects this block to the output layer, to be trainable\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 415,
      "metadata": {
        "id": "8jpEPYifMMuT"
      },
      "outputs": [],
      "source": [
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 416,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQTtt5wNMO_L",
        "outputId": "cd2e9ff6-611b-4e2e-c587-1487731c5e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: tensor([[5211,  345,  423,  640]])\n",
            "Inputs dimensions: torch.Size([1, 4])\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.encode(\"Do you have time\")\n",
        "inputs = torch.tensor(inputs).unsqueeze(0)\n",
        "print(\"Inputs:\", inputs)\n",
        "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 417,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4qYMWXrMSEw",
        "outputId": "86fb45de-f878-4b65-c310-6802812ca0d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs:\n",
            " tensor([[[-1.5854,  0.9904],\n",
            "         [-3.7235,  7.4548],\n",
            "         [-2.2661,  6.6049],\n",
            "         [-3.5983,  3.9902]]])\n",
            "Outputs dimensions: torch.Size([1, 4, 2])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "\n",
        "print(\"Outputs:\\n\", outputs)\n",
        "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 418,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD1yfiw4MT6x",
        "outputId": "40260b88-2b67-4429-e75c-0f62de309702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last output token: tensor([[-3.5983,  3.9902]])\n"
          ]
        }
      ],
      "source": [
        "print(\"Last output token:\", outputs[:, -1, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt65aOLVHvvL"
      },
      "source": [
        "### Classification Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 419,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdKJ0ECAMWI9",
        "outputId": "e584a2b6-66e4-4d78-b906-26d3bcd25bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5.0598e-04, 9.9949e-01]])\n",
            "Class Label: 1\n"
          ]
        }
      ],
      "source": [
        "probas=torch.softmax(outputs[:,-1,:],dim=-1)\n",
        "print(probas)\n",
        "label= torch.argmax(probas)\n",
        "print(\"Class Label:\",label.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hxa1bmqMsln"
      },
      "source": [
        "Using the softmax function here is optional because the largest outputs directly correspond\n",
        "to the highest probability scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 420,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHy1Z6uPLgmS",
        "outputId": "7cfe6326-8ed6-4030-a0a5-c860c4393e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-3.5983,  3.9902]])\n",
            "Class Label: 1\n"
          ]
        }
      ],
      "source": [
        "logits=outputs[:,-1,:]\n",
        "print(logits)\n",
        "label= torch.argmax(probas)\n",
        "print(\"Class Label:\",label.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 421,
      "metadata": {
        "id": "TUyAEQhxMkam"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            num_examples += predicted_labels.shape[0]\n",
        "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
        "        else:\n",
        "            break\n",
        "    return correct_predictions / num_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 422,
      "metadata": {
        "id": "Mw_A7zp3Q-iN"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 423,
      "metadata": {
        "id": "tfMyW85RRBrv"
      },
      "outputs": [],
      "source": [
        "# Same as in chapter 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 424,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O391c1QTSE5Y",
        "outputId": "1abee04c-4ecb-487b-ad03-c8280739302b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 2.183\n",
            "Validation loss: 2.583\n",
            "Test loss: 2.322\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "# num_batches=5 for illustration purpose ,originally dataset have 130 batches\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
        "\n",
        "print(f\"Training loss: {train_loss:.3f}\")\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLORELWZUqnD"
      },
      "source": [
        "Step 1: Set model to training mode\n",
        "\n",
        "Step 2: Reset loss gradients from previous batch iteration\n",
        "\n",
        "Step 3: Calculate loss gradients\n",
        "\n",
        "Step 4: Update model weights using loss gradients\n",
        "\n",
        "Step 5: New: track examples instead of tokens\n",
        "\n",
        "Step 6: Optional evaluation step\n",
        "\n",
        "Step 7: Calculate accuracy after each epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 425,
      "metadata": {
        "id": "Mv471wE_Sa2r"
      },
      "outputs": [],
      "source": [
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                            eval_freq, eval_iter):\n",
        "    # Initialize lists to track losses and examples seen\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Calculate accuracy after each epoch\n",
        "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 426,
      "metadata": {
        "id": "PlVkxPMqUn7Z"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuvCu-OqU87R",
        "outputId": "0eb9647d-3a54-4efa-9ea8-ff05a7c7cdbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n",
            "Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637\n",
            "Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557\n",
            "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
            "Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489\n",
            "Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397\n",
            "Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353\n",
            "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
            "Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320\n",
            "Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306\n",
            "Training accuracy: 90.00% | Validation accuracy: 90.00%\n",
            "Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200\n",
            "Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132\n",
            "Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137\n",
            "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
            "Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143\n",
            "Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074\n",
            "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
            "Training completed in 0.96 minutes.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 428,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqPvxwOcU_I7",
        "outputId": "546fae51-500a-45f8-e589-7f6b0bcd54b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 97.21%\n",
            "Validation accuracy: 97.32%\n",
            "Test accuracy: 95.67%\n"
          ]
        }
      ],
      "source": [
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGWH9OsOqiwx"
      },
      "source": [
        "## USING THE LLM AS A SPAM CLASSIFIER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMjeYgqxqfqZ"
      },
      "source": [
        "Step 1: Prepare inputs to the model\n",
        "\n",
        "Step 2: Truncate sequences if they too long\n",
        "    \n",
        "Step 3: Pad sequences to the longest sequence\n",
        "\n",
        "Step 4: Add batch dimension\n",
        "\n",
        "Step 5: Model inference without gradient tracking\n",
        "    \n",
        "Step 6: Logits of the last output token\n",
        "\n",
        "Step 7: Return the classified result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 429,
      "metadata": {
        "id": "8gfvP-R0nUrR"
      },
      "outputs": [],
      "source": [
        "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare inputs to the model\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    supported_context_length = model.pos_emb.weight.shape[0]\n",
        "    # Note: In the book, this was originally written as pos_emb.weight.shape[1] by mistake\n",
        "    # It didn't break the code but would have caused unnecessary truncation (to 768 instead of 1024)\n",
        "\n",
        "    # Truncate sequences if they too long\n",
        "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
        "\n",
        "    # Pad sequences to the longest sequence\n",
        "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
        "\n",
        "    # Model inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
        "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # Return the classified result\n",
        "    return \"spam\" if predicted_label == 1 else \"not spam\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 430,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_co52NVaqRLQ",
        "outputId": "d1f0be41-de9e-4891-b6f2-c1c8b2d07804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n"
          ]
        }
      ],
      "source": [
        "text_1 = (\n",
        "    \"Your device ID #A93X-77PL has been randomly pre-selected for an unreleased AI assistant trial. 🎉\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPCeZsYEqZkg",
        "outputId": "7bd986a6-c0b1-4049-a8cb-e20f655a4f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not spam\n"
          ]
        }
      ],
      "source": [
        "text_2 = (\n",
        "    \"Hey, just wanted to check if we're still on\"\n",
        "    \" for dinner tonight? Let me know!\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 432,
      "metadata": {
        "id": "OS803wk6Vl-6"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"review_classifier.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 433,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNkztWfdqOY6",
        "outputId": "9df51b7c-369f-45fb-8a6e-8d2ca056dab3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 433
        }
      ],
      "source": [
        "model_state_dict = torch.load(\"review_classifier.pth\")\n",
        "model.load_state_dict(model_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxxYHp4duF-x"
      },
      "source": [
        "# FINETUNING FOR INSTRUCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIu5mE04uFuW",
        "outputId": "d450890e-3c6e-4a7f-d2e4-42c2f034f0ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import urllib\n",
        "import ssl\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "    ssl_context = ssl.create_default_context()\n",
        "    ssl_context.check_hostname = False\n",
        "    ssl_context.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text_data = file.read()\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGLXrri6bti8",
        "outputId": "c6745fdb-f434-4ba3-afdf-b63d38005e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example entry:\n",
            " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
          ]
        }
      ],
      "source": [
        "print(\"Example entry:\\n\", data[50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYpoXZBlbtf-",
        "outputId": "99878d4a-f3cd-47f9-e420-48df5a75f164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Another example entry:\n",
            " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
          ]
        }
      ],
      "source": [
        "print(\"Another example entry:\\n\", data[999])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QYoRjLjbyuu"
      },
      "source": [
        "### CONVERTING INSTRUCTIONS INTO ALPACA FORMAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 437,
      "metadata": {
        "id": "b5NYqDFibtdj"
      },
      "outputs": [],
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 438,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snM6DS6qbtaH",
        "outputId": "75fe4229-2e10-4688-a8f5-62d7d98f4908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify the correct spelling of the following word.\n",
            "\n",
            "### Input:\n",
            "Ocassion\n",
            "\n",
            "### Response:\n",
            "The correct spelling is 'Occasion.'\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'complicated'?\n",
            "\n",
            "### Response:\n",
            "An antonym of 'complicated' is 'simple'.\n"
          ]
        }
      ],
      "source": [
        "model_input = format_input(data[50])\n",
        "\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)\n",
        "\n",
        "model_input = format_input(data[999])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR3NxPFtcVPa"
      },
      "source": [
        "### SPLITTING DATASET INTO TRAIN-TEST-VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 439,
      "metadata": {
        "id": "zWPus_zkbtXY"
      },
      "outputs": [],
      "source": [
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 440,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwMBf4lVbtUt",
        "outputId": "ea907ffb-16bf-44d8-b0ad-612ae97393e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ],
      "source": [
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLVVYeFPc53f"
      },
      "source": [
        "## STEP 2: ORGANIZING DATA INTO TRAINING BATCHES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 441,
      "metadata": {
        "id": "e9j6HMAmbtR1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syEn2r0qmInQ"
      },
      "source": [
        "Custome collate function ,takes a batch ,find maximum,appen end-of-text to shorter inputs ,make sure that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 442,
      "metadata": {
        "id": "NsumYPfPbtPF"
      },
      "outputs": [],
      "source": [
        "def custom_collate_draft_1(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    # and increase the max length by +1, which will add one extra\n",
        "    # padding token below\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    inputs_lst = []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to batch_max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        # Via padded[:-1], we remove the extra padded token\n",
        "        # that has been added via the +1 setting in batch_max_length\n",
        "        # (the extra padding token will be relevant in later codes)\n",
        "        inputs = torch.tensor(padded[:-1])\n",
        "        inputs_lst.append(inputs)\n",
        "\n",
        "    # Convert list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    return inputs_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 443,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxnTlDzNpzd8",
        "outputId": "3a107a88-00f4-471b-f76f-71c49a5e2719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n"
          ]
        }
      ],
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "print(custom_collate_draft_1(batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Q-WKlwp2kB"
      },
      "source": [
        "#### CREATING TARGET TOKEN IDS FOR TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 444,
      "metadata": {
        "id": "I1fe1DCmq47Y"
      },
      "outputs": [],
      "source": [
        "def custom_collate_draft_2(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "    return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 445,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r94z_Ugpzaw",
        "outputId": "367df6a3-b6f1-48e0-ec49-5150b2d802cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256, 50256, 50256, 50256],\n",
            "        [    8,     9, 50256, 50256, 50256]])\n"
          ]
        }
      ],
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "inputs, targets = custom_collate_draft_2(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7wmZmg5rDtq"
      },
      "source": [
        "Replacing with -100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 446,
      "metadata": {
        "id": "oEsaKa6UpzYN"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 447,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmz-BQBCrHQK",
        "outputId": "274ad0eb-e31f-42f4-e5f1-bc15d52d2942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ],
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "inputs, targets = custom_collate_fn(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 448,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIPreVKNrJW1",
        "outputId": "f1e03b3b-66da-4c3e-cf51-da848bc1cb76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is much faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 449,
      "metadata": {
        "id": "cFBwuqKWu0QG"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 450,
      "metadata": {
        "id": "Xh7jRx_lu2tB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 451,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cqDvyOKgu5C7",
        "outputId": "bc43988c-ac2c-402b-9526-60b34640f0c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmqNNYfM-rSc"
      },
      "source": [
        "## STEP 4: LOADING A PRETRAINED LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 452,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf5aoUHbu-St",
        "outputId": "854e8096-bf43-42f0-f746-753c7ad5a5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=model_size,\n",
        "    models_dir=\"gpt2\"\n",
        ")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 453,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FChzJNRpvDJ5",
        "outputId": "1d4eb76c-02f4-420e-ee72-7846d6da6260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "input_text = format_input(val_data[0])\n",
        "print(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 454,
      "metadata": {
        "id": "iMgCSakbvIJr"
      },
      "outputs": [],
      "source": [
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens=35,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256,\n",
        ")\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 455,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uo1pZz2vKVV",
        "outputId": "25bdab5c-3dba-4046-c98f-eac769634acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Convert the active\n"
          ]
        }
      ],
      "source": [
        "response_text = generated_text[len(input_text):].strip()\n",
        "print(response_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 456,
      "metadata": {
        "id": "UTNwfbnuvMdE"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "metadata": {
        "id": "Q3iRhRNOvPYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2747916c-c6e8-4a89-dc57-b91dd7768262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 4.167138862609863\n",
            "Validation loss: 4.050934982299805\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG2ekoEqoAWW",
        "outputId": "3aa817a0-1435-44fc-db64-c583d86517e1"
      },
      "execution_count": 458,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 3.119, Val loss 3.069\n",
            "Ep 1 (Step 000005): Train loss 1.696, Val loss 1.570\n",
            "Ep 1 (Step 000010): Train loss 1.096, Val loss 1.164\n",
            "Ep 1 (Step 000015): Train loss 1.053, Val loss 1.083\n",
            "Ep 1 (Step 000020): Train loss 0.970, Val loss 1.038\n",
            "Ep 1 (Step 000025): Train loss 0.920, Val loss 1.002\n",
            "Ep 1 (Step 000030): Train loss 0.960, Val loss 0.978\n",
            "Ep 1 (Step 000035): Train loss 0.878, Val loss 0.951\n",
            "Ep 1 (Step 000040): Train loss 0.847, Val loss 0.943\n",
            "Ep 1 (Step 000045): Train loss 0.777, Val loss 0.925\n",
            "Ep 1 (Step 000050): Train loss 0.869, Val loss 0.911\n",
            "Ep 1 (Step 000055): Train loss 0.924, Val loss 0.893\n",
            "Ep 1 (Step 000060): Train loss 0.873, Val loss 0.878\n",
            "Ep 1 (Step 000065): Train loss 0.800, Val loss 0.867\n",
            "Ep 1 (Step 000070): Train loss 0.694, Val loss 0.860\n",
            "Ep 1 (Step 000075): Train loss 0.706, Val loss 0.856\n",
            "Ep 1 (Step 000080): Train loss 0.753, Val loss 0.847\n",
            "Ep 1 (Step 000085): Train loss 0.680, Val loss 0.836\n",
            "Ep 1 (Step 000090): Train loss 0.729, Val loss 0.827\n",
            "Ep 1 (Step 000095): Train loss 0.653, Val loss 0.821\n",
            "Ep 1 (Step 000100): Train loss 0.634, Val loss 0.808\n",
            "Ep 1 (Step 000105): Train loss 0.728, Val loss 0.803\n",
            "Ep 1 (Step 000110): Train loss 0.719, Val loss 0.799\n",
            "Ep 1 (Step 000115): Train loss 0.672, Val loss 0.796\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: The following is an instruction that describes a task.\n",
            "Training completed in 0.59 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "_3zoqL1DVYMp"
      },
      "execution_count": 459,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "wUCSryNkVbcY",
        "outputId": "1e1d6702-e30b-4301-cc84-e7e99813929a"
      },
      "execution_count": 460,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 6: EXTRACTING AND SAVING RESPONSES"
      ],
      "metadata": {
        "id": "CLatiKWWlkak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "for entry in test_data[:3]:\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        ")\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK7NrzCMoM3R",
        "outputId": "d2f21f7e-a376-4b7c-a1ed-2d1e1cc932c5"
      },
      "execution_count": 461,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence using a simile.\n",
            "\n",
            "### Input:\n",
            "The car is very fast.\n",
            "\n",
            "Correct response:\n",
            ">> The car is as fast as lightning.\n",
            "\n",
            "Model response:\n",
            ">> The car is very fast.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What type of cloud is typically associated with thunderstorms?\n",
            "\n",
            "Correct response:\n",
            ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
            "\n",
            "Model response:\n",
            ">> A type of cloud is typically associated with thunderstorms.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name the author of 'Pride and Prejudice'.\n",
            "\n",
            "Correct response:\n",
            ">> Jane Austen.\n",
            "\n",
            "Model response:\n",
            ">> The author of 'Pride and Prejudice' is Robert Frost.\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most importantly, we can see that model evaluation is not as straightforward as in the\n",
        "previous chapter, where we simply calculated the percentage of correct spam/non-spam\n",
        "class labels to obtain the classification accuracy.\n",
        "\n",
        "In practice, instruction-finetuned LLMs\n",
        "such as chatbots are evaluated via multiple approaches:\n",
        "\n",
        "1. Short-answer and multiple choice benchmarks such as MMLU (\"Measuring\n",
        "Massive Multitask Language Understanding,\" https://arxiv.org/abs/2009.\n",
        "03300), which test the general knowledge of a model.\n",
        "\n",
        "2. Human preference comparison to other LLMs, such as LMSYS chatbot\n",
        "arena (https://arena.lmsys.org).\n",
        "\n",
        "3. Automated conversational benchmarks, where another LLM like GPT-4 is\n",
        "used to evaluate the responses, such as AlpacaEval (https://tatsulab.github.io/alpaca_eval/).\n",
        "completes the request."
      ],
      "metadata": {
        "id": "1r1UrRBB26Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLVWKpBUliPZ",
        "outputId": "77508da8-c9ca-48f2-8946-e96f226a780e"
      },
      "execution_count": 462,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [00:36<00:00,  3.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
        "torch.save(model.state_dict(), file_name)\n",
        "print(f\"Model saved as {file_name}\")\n",
        "\n",
        "# Load model via\n",
        "model.load_state_dict(torch.load(\"gpt2-small124M-sft.pth\"))"
      ],
      "metadata": {
        "id": "GfithZPvlqZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97b6911-df68-4377-80c1-cdad8998e87f"
      },
      "execution_count": 463,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as gpt2-small124M-sft.pth\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 463
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## USE THE LLM AS PERSONAL ASSISTANT"
      ],
      "metadata": {
        "id": "81guxOgjH1cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample instruction:\"\"\n",
        "entry = {\n",
        "     \"instruction\": \"Explain the importance of data structures in programming.\",\n",
        "     \"input\": \"\"\n",
        "     }\n",
        "\n",
        "input_text = format_input(entry)\n",
        "idx = text_to_token_ids(input_text, tokenizer).to(device)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=idx,\n",
        "    max_new_tokens=256,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    temperature=0.5,\n",
        "    top_k=None,\n",
        "    eos_id=50256\n",
        "    )\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "response_text = ( generated_text[len(input_text):] .replace(\"### Response:\", \"\") .strip() )\n",
        "print(\"Prompt:\\n\", input_text)\n",
        "print(\"\\nModel response:\\n>>\", response_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwKs1W7UeDL1",
        "outputId": "cb964c0e-285f-4adb-8896-a25f598d4337"
      },
      "execution_count": 470,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Explain the importance of data structures in programming.\n",
            "\n",
            "Model response:\n",
            ">> Data structures are the means by which data is stored, processed, and used. Data structures are the means by which information is applied to a task.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZtVhkRanhf_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 7: EVALUATING THE FINE-TUNED LLM"
      ],
      "metadata": {
        "id": "YZcNiS-_mBDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MMLU Test"
      ],
      "metadata": {
        "id": "dTjuGTV72Fwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ollama Evaluation (I am not evaluating model because of hardware limitations. )"
      ],
      "metadata": {
        "id": "waSL2WhwQsb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the evaluation step which involves evaluating test set responses in\n",
        "an automated fashion, we utilize an existing instruction-finetuned 8 billion parameter Llama\n",
        "3 model developed by Meta AI.\n",
        "\n",
        "This model can be run locally using the open-source Ollama\n",
        "application (https://ollama.com)."
      ],
      "metadata": {
        "id": "RPBU-SjL3po4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jZRZ3cVU9HWw"
      },
      "execution_count": 464,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwvgVFcal66l"
      },
      "execution_count": 464,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mf_vdDdomDP6"
      },
      "execution_count": 464,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "okiOdAnemFj6"
      },
      "execution_count": 464,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}